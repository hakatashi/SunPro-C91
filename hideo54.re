= μ'sメンバーの声を学習して誰が歌っているかを当てる

//raw[|latex|\\chapterauthor{hideo54}]

== はじめに

こんにちは、hideo54です。灘高校で高校2年生をしています。最近の口癖は「受験したくない」です。これまでずっとSunPro会誌に記事を出してきましたが、もしかすると高校生でSunPro会誌の記事を執筆するのはこれが最後かもしれない...?!

さて、僕は今年1月のNHKでの再放送をきっかけとして今更ラブライブ!にハマってしまったわけですが、この記事ではタイトル通り、μ'sのメンバーの声を学習し、誰が歌っているのか判別できるようにした話をします。

この手の分野に対して全くの素人である僕がやる気になった動機として、学校の物理の授業で音について学習した際、先生がPCを持ち込んで、声の波形や、人によって波形が異なる様子を実演してくれたことで、なんとなくしか知らなかった、頑張ったら音を機械的に解析できそうだということが実感できたという出来事があります。
そしてアニソンを聞いていて、歌っている人を識別できたら面白そうだなと思いました。「アニメ声」という言葉が存在するように、アニメの声は特徴的な声が多く、識別が容易そうな気がします。

数ある声優ユニットの中からμ'sを選んだのには、以下の理由があります:

    * 1人1人の声が割りと異なるので識別が容易そう
    * サンプル音声の収集が楽そう
        ** 楽曲の数が非常に多い@<fn>{many-songs}
        ** そのうちのだいたいはiTunesで試聴できる
        ** メンバー1人のみが歌っている「ラブライブ! Solo Live!」というアルバムが各メンバーごとにある

//footnote[many-songs][@<href>{http://www.lovelive-anime.jp/otonokizaka/release.html}]

というわけで、次の節から、実際に解析を始めてみようと思います! ただ、前述の通り僕はまだ初心者なので、読者の方にこの道のプロの方がいらっしゃれば、至らぬ点を見つけたらこの記事の終わりの方に書いてある連絡先の方に気軽に斧を投げていただければ幸いです。

また、以降の記事では、以下を利用しました:

    * Python 3.5.2
    * numpy (言わずと知れた計算お役立ちライブラリ)
    * pylab (グラフ描画用にmatplotlibを使用するためのモジュール)
    * python_speech_features@<fn>{python_speech_features} (音声処理用ライブラリ)
    * requests (サンプル集める用にAPIを叩く時に使用)
    * scikit-learn (機械学習用)

//footnote[python_speech_features][@<href>{https://github.com/jameslyons/python_speech_features}]

これらのインストールに必要な過程は省略します。

== どうやんの?

高校物理で習うように、音(音波)は波動の1つです。声の高さは振動数に、大きさは振れ幅に、特徴は波形に現れます。

たとえば、「あー」という声の波形を見てみましょう。ここでは、アニメ1期第4話「まきりんぱな」での、小泉花陽さんと西木野真姫さんの発声練習の部分から短時間分を切り抜いた声を見てみます。次のようなPythonコードを書きました:

//source[sources/hideo54/a.py]{
//}

切り取りの都合で横軸を15000からにしてます。以上を実行して生成される画像が以下になります。

//image[a][「あー」の波形]

確かに、波形が異なるものの、どちらも周期的な波であるということが確認できますね。(小泉花陽さんが小さめに歌ってる様子も振れ幅からわかります)

この波形(=声の特徴)を扱いやすくするために、メル周波数ケプストラム係数(MFCC)というものを使うと良いらしいです。波形をフーリエ変換することで得られるスペクトル@<fn>{fyi}の対数値をさらにフーリエ変換して得られたケプストラムを、メル尺度を使って変換したものだとか。なるほどわからん。そこで、超お手軽にMFCCを計算できるライブラリ"python_speech_features"を使ってみます。(これ使わずにnumpyで一からやろうとしたけど挫折した。)

//footnote[fyi][ちなみに、「はじめに」で述べた、学校の物理の授業で見せてもらったのはこれです。]

//source[sources/hideo54/mfcc.py]{
//}

超簡単ですね。とりあえず、様々な曲からMFCCを得るため、次の節でひとまずサンプルを集めます。

== 各メンバーの声を集める

前述の通り、各メンバーがソロで歌っている「ラブライブ! Solo Live!」というアルバムがあります。そのうち全アルバムに共通している曲(全員のソロが用意されている曲)は以下の11曲です:

    * もぎゅっと"love"で接近中!
    * 愛してるばんざーい!
    * Wonderful Rush
    * Oh,Love&Peace!
    * 僕らは今のなかで
    * WILD STARS
    * きっと青春が聞こえる
    * 輝夜の城で踊りたい
    * Wonder zone
    * No brand girls
    * START:DASH!!

これらのうち、No brand girlsを除いた10曲の試聴用音声(30秒@<fn>{why-30s})を、iTunes Search API@<fn>{itunes-search-api}を利用して落としてきます。

//footnote[why-30s][iTunes Storeだと90秒の試聴ができるんですが、公開されているAPIだと30秒しか落とせないそうです。(Apple社員の人がstack overflowで言ってた @<href>{http://stackoverflow.com/a/14620405} 。)悲しいなあ。]
//footnote[itunes-search-api][@<href>{https://affiliate.itunes.apple.com/resources/documentation/itunes_search_api_jp/}]

//source[sources/hideo54/solo-live.py]{
//}

よくわからないのですが、たまにタイムアウトするので、タイムアウトした分はもう1度落とすようにしています。それでも無理なものは数分後くらいにタイムアウトが解消されたら適当にcurlとかで拾ってやります。

これで、各メンバーにつき10曲の30秒試聴用音声がダウンロードされました。ありがたいことに試聴部分は曲ごとにすべて統一されています。

後で扱いやすいよう、以下のシェルスクリプトを使って、ffmpegでこれをwavに変換します。その際ファイル名に@<tt>{!}や@<tt>{"}, @<tt>{:}, スペースなどが入っていると非常に扱いづらくなるので、事前にそちらの変更もして、ついでに見やすくするためにメンバーごとにディレクトリを分けておきます。

//source[sources/towav.sh]{
//}

#@# 以降執筆中

== 学習させてみる

== おわりに
